{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e6f6c9-4b97-4198-aeff-262efd32b23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2r4r.ent' Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2r4s.ent' \n",
      "\n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2rh1.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3d4s.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3nya.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3ny9.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3kj6.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3ny8.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3pds.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4ldl.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3p0g.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4gbr.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5x7d.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4qkx.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4lde.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4ldo.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5d5a.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6n48.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6e67.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5d6l.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5d5b.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6mxt.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6oba.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps5.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps4.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps1.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps2.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps3.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6prz.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps0.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps6.ent' \n",
      "Reordered TMH ends for 2r4r: [39, 59, 110, 68, 112, 135, 203, 147, 205, 225, 316, 271, 317, 327]\n",
      "Reordered TMH ends for 2r4s: [39, 59, 110, 68, 112, 135, 203, 147, 205, 230, 316, 271, 317, 327]\n",
      "Reordered TMH ends for 2rh1: [31, 60, 96, 67, 102, 136, 172, 147, 197, 229, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 3d4s: [33, 60, 95, 67, 102, 136, 172, 147, 197, 229, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 3kj6: [36, 59, 110, 67, 111, 130, 161, 147, 163, 228, 312, 271, 314, 328]\n",
      "Reordered TMH ends for 3ny8: [33, 60, 95, 67, 103, 135, 172, 147, 197, 228, 296, 267, 305, 328]\n",
      "Reordered TMH ends for 3ny9: [33, 60, 96, 67, 102, 136, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 3nya: [33, 60, 95, 67, 102, 135, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 3p0g: [29, 60, 95, 67, 102, 136, 171, 147, 197, 226, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 3pds: [32, 60, 95, 67, 103, 135, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 4gbr: [30, 60, 95, 67, 103, 135, 171, 147, 197, 230, 270, 239, 277, 300]\n",
      "Reordered TMH ends for 4lde: [1024, 1060, 1096, 1067, 1102, 1136, 1170, 1147, 1197, 1225, 1299, 1267, 1305, 1327]\n",
      "Reordered TMH ends for 4ldl: [1024, 1060, 1096, 1067, 1102, 1136, 1170, 1147, 1197, 1225, 1299, 1267, 1305, 1327]\n",
      "Reordered TMH ends for 4ldo: [1027, 1060, 1096, 1067, 1103, 1136, 1170, 1147, 1197, 1228, 1299, 1267, 1305, 1327]\n",
      "Reordered TMH ends for 4qkx: [1028, 1060, 1096, 1067, 1103, 1136, 1171, 1147, 1197, 1225, 1298, 1267, 1305, 1328]\n",
      "Reordered TMH ends for 5d5a: [31, 60, 96, 67, 102, 135, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 5d5b: [31, 60, 95, 67, 102, 135, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 5d6l: [31, 60, 95, 67, 102, 136, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 5x7d: [33, 60, 95, 67, 102, 136, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 6e67: [30, 59, 95, 67, 103, 136, 172, 147, 197, 230, 298, 268, 305, 328]\n",
      "Reordered TMH ends for 6mxt: [162, 1060, 1096, 1067, 1103, 1135, 1170, 1147, 1197, 1228, 1299, 1267, 1305, 1327]\n",
      "Reordered TMH ends for 6n48: [1024, 1060, 1096, 1067, 1103, 1136, 1170, 1147, 1197, 1225, 1299, 1267, 1305, 1327]\n",
      "Reordered TMH ends for 6oba: [33, 60, 96, 67, 102, 136, 172, 147, 197, 228, 299, 267, 305, 328]\n",
      "Reordered TMH ends for 6prz: [32, 60, 95, 67, 102, 135, 172, 147, 197, 228, 298, 267, 306, 328]\n",
      "Reordered TMH ends for 6ps0: [32, 60, 95, 67, 102, 136, 172, 147, 197, 228, 298, 267, 304, 328]\n",
      "Reordered TMH ends for 6ps1: [30, 60, 95, 67, 103, 136, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 6ps2: [30, 60, 95, 67, 102, 135, 172, 147, 197, 228, 298, 267, 306, 328]\n",
      "Reordered TMH ends for 6ps3: [30, 60, 96, 67, 102, 135, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 6ps4: [30, 60, 95, 67, 102, 135, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 6ps5: [30, 60, 95, 67, 102, 135, 172, 147, 197, 228, 298, 267, 305, 328]\n",
      "Reordered TMH ends for 6ps6: [30, 60, 95, 67, 102, 135, 172, 147, 199, 228, 296, 267, 305, 328]\n",
      "Downloading PDB structure '2r4r'...\n",
      "Downloading PDB structure '2r4s'...\n",
      "Downloading PDB structure '2rh1'...\n",
      "Downloading PDB structure '3d4s'...\n",
      "Downloading PDB structure '3kj6'...\n",
      "Downloading PDB structure '3ny8'...\n",
      "Downloading PDB structure '3ny9'...\n",
      "Downloading PDB structure '3nya'...\n",
      "Downloading PDB structure '3p0g'...\n",
      "Downloading PDB structure '3pds'...\n",
      "Downloading PDB structure '4gbr'...\n",
      "Downloading PDB structure '4lde'...\n",
      "Downloading PDB structure '4ldl'...\n",
      "Downloading PDB structure '4ldo'...\n",
      "Downloading PDB structure '4qkx'...\n",
      "Downloading PDB structure '5d5a'...\n",
      "Downloading PDB structure '5d5b'...\n",
      "Downloading PDB structure '5d6l'...\n",
      "Downloading PDB structure '5x7d'...\n",
      "Downloading PDB structure '6e67'...\n",
      "Downloading PDB structure '6mxt'...\n",
      "Downloading PDB structure '6n48'...\n",
      "Downloading PDB structure '6oba'...\n",
      "Downloading PDB structure '6prz'...\n",
      "Downloading PDB structure '6ps0'...\n",
      "Downloading PDB structure '6ps1'...\n",
      "Downloading PDB structure '6ps2'...\n",
      "Downloading PDB structure '6ps3'...\n",
      "Downloading PDB structure '6ps4'...\n",
      "Downloading PDB structure '6ps5'...\n",
      "Downloading PDB structure '6ps6'...\n",
      "Downloading PDB structure '6ps6'...\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from Bio import PDB, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pydssp\n",
    "import math\n",
    "\n",
    "# Pick PDB structures\n",
    "pdb_codes = [\n",
    "\"2r4r\",\n",
    "\"2r4s\",\n",
    "\"2rh1\",\n",
    "\"3d4s\",\n",
    "\"3kj6\",\n",
    "\"3ny8\",\n",
    "\"3ny9\",\n",
    "\"3nya\",\n",
    "\"3p0g\",\n",
    "\"3pds\",\n",
    "\"4gbr\",\n",
    "\"4lde\",\n",
    "\"4ldl\",\n",
    "\"4ldo\",\n",
    "\"4qkx\",\n",
    "\"5d5a\",\n",
    "\"5d5b\",\n",
    "\"5d6l\",\n",
    "\"5x7d\",\n",
    "\"6e67\",\n",
    "\"6mxt\",\n",
    "\"6n48\",\n",
    "\"6oba\",\n",
    "\"6prz\",\n",
    "\"6ps0\",\n",
    "\"6ps1\",\n",
    "\"6ps2\",\n",
    "\"6ps3\",\n",
    "\"6ps4\",\n",
    "\"6ps5\",\n",
    "\"6ps6\"\n",
    "]\n",
    "\n",
    "# Download PDB files\n",
    "def download_pdb(pdb_code, save_dir):\n",
    "    pdb_file = PDB.PDBList()\n",
    "    pdb_file.retrieve_pdb_file(pdb_code, file_format=\"pdb\", pdir=save_dir, overwrite=False)\n",
    "\n",
    "def download_all_pdbs(pdb_codes, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_pdb, pdb_codes, [save_dir] * len(pdb_codes))\n",
    "\n",
    "\n",
    "# Convert amino acids for FASTA format\n",
    "AA_DICT = {\n",
    "    \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "    \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "    \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "    \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\"\n",
    "}\n",
    "\n",
    "def three_to_one(resname):\n",
    "    return AA_DICT.get(resname, \"NA\")\n",
    "\n",
    "\n",
    "# Extract sequence and write to FASTA\n",
    "def extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir):\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    fasta_filepath = os.path.join(fasta_dir, f\"{pdb_code}.fasta\")\n",
    "\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "\n",
    "    model = structure[0]\n",
    "    sequence, observed_residues = [], []\n",
    "\n",
    "    chain = model[chain_id] if chain_id in model else None\n",
    "    if chain:\n",
    "        for residue in chain.get_residues():\n",
    "            if PDB.is_aa(residue):\n",
    "                sequence.append(three_to_one(residue.get_resname()))\n",
    "                observed_residues.append(residue.id[1])\n",
    "\n",
    "    # Write FASTA\n",
    "    fasta_seq = SeqRecord(Seq(\"\".join(sequence)), id=f\"{pdb_code}_{chain_id}\", description=\"\")\n",
    "    SeqIO.write(fasta_seq, fasta_filepath, \"fasta\")\n",
    "\n",
    "    return observed_residues\n",
    "\n",
    "# Run DeepTMHMM using WSL\n",
    "def run_deeptmhmm(pdb_code, fasta_filepath_wsl):\n",
    "    pdb_results_dir = f\"/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Project_automation/Python/DeepTMHMM_results/{pdb_code}\"\n",
    "    os.makedirs(pdb_results_dir, exist_ok=True)\n",
    "\n",
    "    process = subprocess.run(\n",
    "        [\"wsl\", \"/home/dan/.local/bin/biolib\", \"run\", \"--local\", \"DTU/DeepTMHMM:1.0.24\", \"--fasta\", f\"{fasta_filepath_wsl}\"],\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        cwd=pdb_results_dir\n",
    "    )   \n",
    "    return pdb_code\n",
    "\n",
    "# Save only the TMR file of the DeepTMHMM output\n",
    "def keep_only_tmr(results_dir, pdb_codes):\n",
    "    for pdb_code in pdb_codes:\n",
    "        pdb_results_dir = os.path.join(results_dir, pdb_code, \"biolib_results\")\n",
    "\n",
    "        if os.path.exists(pdb_results_dir):\n",
    "            for filename in os.listdir(pdb_results_dir):\n",
    "                file_path = os.path.join(pdb_results_dir, filename)\n",
    "\n",
    "                if filename != \"TMRs.gff3\":\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not remove {file_path}: {e}\")\n",
    "\n",
    "# Extract coordinates from PDB files and convert to PyTorch tensor for DSSP\n",
    "def extract_coordinates(pdb_code, pdb_dir, chain_id='A'):\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "    \n",
    "    model = structure[0]\n",
    "    chain = model[chain_id]\n",
    "\n",
    "    coordinates = []\n",
    "    for residue in chain:\n",
    "        if PDB.is_aa(residue):\n",
    "            for atom in residue:\n",
    "                if atom.get_name() in ['N', 'CA', 'C', 'O']:\n",
    "                    coordinates.append(atom.coord)\n",
    "                    \n",
    "    # Convert coordinates to numpy array and then PyTorch tensor\n",
    "    coord_array = np.array(coordinates)\n",
    "    L = sum(1 for residue in chain if PDB.is_aa(residue))\n",
    "    atoms = 4  # N, CA, C, O\n",
    "    xyz = 3  # x, y, z coordinates\n",
    "    \n",
    "    coord_tensor = torch.tensor(coord_array, dtype=torch.float32).reshape([L, atoms, xyz])\n",
    "    \n",
    "    return coord_tensor\n",
    "\n",
    "\n",
    "# Calculate the amount of residues DSSP assigns a 'H' (the desired extension)\n",
    "def calculate_desired_extensions(tmh_ranges, ss_data, max_extend=12):\n",
    "    desired_extensions = []\n",
    "\n",
    "    for start, end in tmh_ranges:\n",
    "        if np.isnan(start) or np.isnan(end):\n",
    "            desired_extensions.append((0, 0))\n",
    "            continue\n",
    "        \n",
    "        # Backward extension\n",
    "        backward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            ss_index = start - i\n",
    "            if ss_index >= 0 and ss_data[ss_index] == \"H\":\n",
    "                backward_extension += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Forward extension\n",
    "        forward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            ss_index = end + i\n",
    "            if ss_index < len(ss_data) and ss_data[ss_index] == \"H\":\n",
    "                forward_extension += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        desired_extensions.append((backward_extension, forward_extension))\n",
    "\n",
    "    return desired_extensions\n",
    "\n",
    "# Calculate the residues between the helix to be extended and the next helix in that direction\n",
    "def calculate_available_spaces(tmh_ranges):\n",
    "    available_spaces = []\n",
    "\n",
    "    for i in range(len(tmh_ranges) - 1):\n",
    "        prev_end = tmh_ranges[i][1]\n",
    "        next_start = tmh_ranges[i + 1][0]\n",
    "        available_space = next_start - prev_end - 1\n",
    "        available_spaces.append(available_space)\n",
    "\n",
    "    return available_spaces\n",
    "\n",
    "\n",
    "# Reorder TMH ends to alternate between extra and intra\n",
    "def reorder_gpcr_tmh_ends(tmh_extended_pairs):\n",
    "    pattern = [\"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\", \"intra\", \n",
    "               \"extra\", \"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\"]\n",
    "\n",
    "    reordered = []\n",
    "    for i, label in enumerate(pattern):\n",
    "        if label == \"extra\":\n",
    "            reordered.append(tmh_extended_pairs[i // 2][0])\n",
    "        else:\n",
    "            reordered.append(tmh_extended_pairs[i // 2][1])\n",
    "\n",
    "    return reordered\n",
    "\n",
    "# Run all the functions created\n",
    "def main():\n",
    "    # Assign directories\n",
    "    pdb_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\"\n",
    "    fasta_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\Fasta_files\"\n",
    "    results_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\DeepTMHMM_results\"\n",
    "    \n",
    "    # Download PDB files\n",
    "    download_all_pdbs(pdb_codes, pdb_dir)\n",
    "\n",
    "    # Extract sequences\n",
    "    chain_id = \"A\"\n",
    "    all_observed_residues = {\n",
    "        pdb_code: extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir) for pdb_code in pdb_codes\n",
    "    }\n",
    "\n",
    "    # Run DeepTMHMM\n",
    "    for pdb_code in pdb_codes:\n",
    "        fasta_filepath_wsl = f\"/mnt/c/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Project_automation/Python/Fasta_files/{pdb_code}.fasta\"  \n",
    "        run_deeptmhmm(pdb_code, fasta_filepath_wsl)\n",
    "        \n",
    "    keep_only_tmr(results_dir, pdb_codes)\n",
    "\n",
    "    pdb_data = {}\n",
    "\n",
    "    # Extract TMH_ranges from DeepTMHMM output file\n",
    "    for pdb_code in pdb_codes:\n",
    "        tmh_ranges = []\n",
    "        tmh_result_file = os.path.join(results_dir, pdb_code, \"biolib_results\", \"TMRs.gff3\")\n",
    "\n",
    "        if os.path.exists(tmh_result_file):\n",
    "            with open(tmh_result_file) as file:\n",
    "                for line in file:\n",
    "                    if \"TMhelix\" in line:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        start, end = int(parts[2]), int(parts[3])\n",
    "                        tmh_ranges.append((start, end))\n",
    "                        \n",
    "        # Edit readings that pick up 8 helices                \n",
    "        if len(tmh_ranges) == 8:\n",
    "            tmh_1_2 = [tmh_ranges[0][0], tmh_ranges[1][1]]\n",
    "            tmh_ranges = [tmh_1_2] + tmh_ranges[2:]                    \n",
    "\n",
    "        # Assign warnings to readings of less than 7 helices\n",
    "        if len(tmh_ranges) < 7:\n",
    "            print(f\"⚠️ Warning: {pdb_code} has only {len(tmh_ranges)} TMHs detected\")\n",
    "            tmh_ranges += [(np.nan, np.nan)] * (7 - len(tmh_ranges))\n",
    "\n",
    "        # Extract secondary structure using DSSP\n",
    "        pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "        coord_tensor = extract_coordinates(pdb_code, pdb_dir, chain_id)\n",
    "        ss_data = pydssp.assign(coord_tensor, out_type='c3')\n",
    "\n",
    "        # Compute desired extensions\n",
    "        desired_extensions = calculate_desired_extensions(tmh_ranges, ss_data)\n",
    "        available_spaces = calculate_available_spaces(tmh_ranges)\n",
    "        max_extension = 9\n",
    "\n",
    "        desired_extensions = [\n",
    "            (min(start, max_extension), min(end, max_extension)) for start, end in desired_extensions\n",
    "        ]\n",
    "\n",
    "        # Align extensions with available spaces\n",
    "        desired_extensions_dict = dict(enumerate(desired_extensions))\n",
    "        for i in range(len(available_spaces)):\n",
    "            total_desired = desired_extensions_dict[i][1] + desired_extensions_dict[i + 1][0]\n",
    "            available = available_spaces[i]\n",
    "\n",
    "            if total_desired > available:\n",
    "                half_space = math.floor(available / 2)\n",
    "                if half_space >= desired_extensions_dict[i][1]:\n",
    "                    desired_extensions_dict[i + 1] = (available - desired_extensions_dict[i][1], desired_extensions_dict[i + 1][1])\n",
    "                elif half_space >= desired_extensions_dict[i + 1][0]:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], available - desired_extensions_dict[i + 1][0])\n",
    "                else:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], half_space)\n",
    "                    desired_extensions_dict[i + 1] = (half_space, desired_extensions_dict[i + 1][1])\n",
    "\n",
    "        # Make extensions\n",
    "        extended_tmh_ranges = []\n",
    "        for i, (start, end) in enumerate(tmh_ranges):\n",
    "            left_extension = desired_extensions_dict[i][0]\n",
    "            right_extension = desired_extensions_dict[i][1]\n",
    "\n",
    "            if np.isnan(start) or np.isnan(end):\n",
    "                new_start, new_end = np.nan, np.nan  \n",
    "            else:\n",
    "                new_start = start - left_extension\n",
    "                new_end = end + right_extension\n",
    "\n",
    "            extended_tmh_ranges.append((new_start, new_end))\n",
    "\n",
    "        # Reorder TMH ends\n",
    "        tmh_extended_pairs = [\n",
    "            (all_observed_residues[pdb_code][int(start)] if not np.isnan(start) else np.nan,\n",
    "             all_observed_residues[pdb_code][int(end)] if not np.isnan(end) else np.nan)\n",
    "            for start, end in extended_tmh_ranges\n",
    "        ]\n",
    "\n",
    "        reordered_tmh_ends = reorder_gpcr_tmh_ends(tmh_extended_pairs)\n",
    "        print(f\"Reordered TMH ends for {pdb_code}: {reordered_tmh_ends}\")\n",
    "\n",
    "        # Store reordered_tmh_ends in pdb_data dictionary\n",
    "        pdb_data[pdb_code] = {\"A\": reordered_tmh_ends}\n",
    "\n",
    "    return pdb_data  # Return final dictionary\n",
    "\n",
    "# Run main() and store result in a variable\n",
    "if __name__ == \"__main__\":\n",
    "    pdb_data = main()\n",
    "\n",
    "\n",
    "# Create function to extract coordinates\n",
    "def get_coords(pdb_id, chain_id, residues):\n",
    "    pdb_id = pdb_id.lower()\n",
    "    pdbl = PDB.PDBList()\n",
    "    coords = [pdb_id.upper(), chain_id] \n",
    "\n",
    "    # Create temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        pdb_file_path = pdbl.retrieve_pdb_file(pdb_id, pdir=temp_dir, file_format=\"pdb\")\n",
    "\n",
    "        # Parse the file\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(pdb_id, pdb_file_path)\n",
    "\n",
    "        # Extract resolution\n",
    "        resolution = \"Unknown\"\n",
    "        with open(pdb_file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"REMARK   2 RESOLUTION\"):\n",
    "                    resolution = line.split()[3]\n",
    "                    break\n",
    "\n",
    "        coords.insert(1, resolution) \n",
    "        \n",
    "        # Loop through file to extract residue coordinates\n",
    "        for residue_id in residues:\n",
    "            found = False\n",
    "            for model in structure:\n",
    "                if chain_id in model:\n",
    "                    chain = model[chain_id]\n",
    "                    if residue_id in chain:\n",
    "                        residue = chain[residue_id]\n",
    "                        residue_name = residue.get_resname()\n",
    "                        for atom in residue:\n",
    "                            if atom.get_name() == \"CA\":\n",
    "                                coords.extend([f\"{residue_name}{residue_id}\", *atom.coord])\n",
    "                                found = True\n",
    "                                break\n",
    "            if not found:\n",
    "                coords.extend([f\"Unknown{residue_id}\", \"NA\", \"NA\", \"NA\"])\n",
    "\n",
    "    return coords\n",
    "\n",
    "data = []\n",
    "\n",
    "# Collect and prepare items to be processed by function\n",
    "for pdb_id, chains in pdb_data.items():\n",
    "    for chain_id, residues in chains.items():\n",
    "        data.append(get_coords(pdb_id, chain_id, residues))\n",
    "\n",
    "if not pdb_data or all(not chains for chains in pdb_data.values()):\n",
    "    raise ValueError(\"pdb_data is empty or contains no residue information.\")\n",
    "\n",
    "\n",
    "# Organise the data frame for accurate conversion to Excel\n",
    "max_residues = max(len(residues) for chains in pdb_data.values() for residues in chains.values())\n",
    "\n",
    "# Define columns dynamically\n",
    "columns = [\"PDB ID\", \"Resolution\", \"Chain\"] + sum([[\"Res\", \"X\", \"Y\", \"Z\"]] * max_residues, [])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save as Excel output\n",
    "output_file = \"C:/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Landmarks/Automated landmarks/Protein_coordinates.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# print(pdb_data)\n",
    "\n",
    "#Print coordinates as a test\n",
    "coordinates = get_coords(pdb_id, chain_id, residues)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
