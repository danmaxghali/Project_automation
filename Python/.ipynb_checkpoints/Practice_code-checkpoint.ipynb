{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9f6995-4404-40e3-89be-ba95a40501f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6e67.ent' Downloading PDB structure '4gbr'...\n",
      "\n",
      "Downloading PDB structure '5d5a'...\n",
      "Downloading PDB structure '7dhi'...\n",
      "[6e67] STDOUT: 2025-03-05 00:31:15,654 | INFO : Extracted zip file to: output/\n",
      "\n",
      "2025-03-05 00:31:15,654 | INFO : Done in 2.64 seconds\n",
      "\n",
      "2025-03-05 00:31:15,654 | INFO : Extracted zip file to: output/\n",
      "2025-03-05 00:31:15,654 | INFO : Done in 2.64 seconds\n",
      "\n",
      "[6e67] STDERR: \n",
      "[4gbr] STDOUT: 2025-03-05 00:31:32,562 | INFO : Extracted zip file to: output/\n",
      "\n",
      "2025-03-05 00:31:32,565 | INFO : Done in 2.76 seconds\n",
      "\n",
      "2025-03-05 00:31:32,562 | INFO : Extracted zip file to: output/\n",
      "2025-03-05 00:31:32,565 | INFO : Done in 2.76 seconds\n",
      "\n",
      "[4gbr] STDERR: \n",
      "[5d5a] STDOUT: 2025-03-05 00:31:26,760 | INFO : Extracted zip file to: output/\n",
      "\n",
      "2025-03-05 00:31:26,760 | INFO : Done in 2.54 seconds\n",
      "\n",
      "2025-03-05 00:31:26,760 | INFO : Extracted zip file to: output/\n",
      "2025-03-05 00:31:26,760 | INFO : Done in 2.54 seconds\n",
      "\n",
      "[5d5a] STDERR: \n",
      "[7dhi] STDOUT: 2025-03-05 00:31:06,099 | INFO : Extracted zip file to: output/\n",
      "\n",
      "2025-03-05 00:31:06,100 | INFO : Done in 1.52 seconds\n",
      "\n",
      "\n",
      "2025-03-05 00:31:06,099 | INFO : Extracted zip file to: output/\n",
      "2025-03-05 00:31:06,100 | INFO : Done in 1.52 seconds\n",
      "\n",
      "[7dhi] STDERR: \n",
      "\n",
      "Reordered TMH ends for 6e67: [29, 58, 94, 66, 102, 135, 171, 146, 196, 228, 297, 3003, 304, 327]\n",
      "Reordered TMH ends for 4gbr: [29, 59, 94, 66, 102, 134, 170, 146, 196, 229, 269, 238, 276, 299]\n",
      "Reordered TMH ends for 5d5a: [30, 59, 95, 66, 101, 134, 171, 146, 196, 227, 297, 266, 304, 327]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 284\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReordered TMH ends for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdb_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreordered_tmh_ends\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 279\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Reorder TMH ends\u001b[39;00m\n\u001b[0;32m    275\u001b[0m tmh_extended_pairs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    276\u001b[0m     (all_observed_residues[pdb_code][start \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], all_observed_residues[pdb_code][end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m extended_tmh_ranges\n\u001b[0;32m    278\u001b[0m ]\n\u001b[1;32m--> 279\u001b[0m reordered_tmh_ends \u001b[38;5;241m=\u001b[39m \u001b[43mreorder_gpcr_tmh_ends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmh_extended_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReordered TMH ends for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdb_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreordered_tmh_ends\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 196\u001b[0m, in \u001b[0;36mreorder_gpcr_tmh_ends\u001b[1;34m(tmh_extended_pairs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pattern):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 196\u001b[0m         reordered\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtmh_extended_pairs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Take start residue\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# \"intra\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         reordered\u001b[38;5;241m.\u001b[39mappend(tmh_extended_pairs[i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Take end residue\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from Bio import PDB, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pydssp\n",
    "import math\n",
    "\n",
    "# === 1️⃣ Parallel PDB Downloading ===\n",
    "def download_pdb(pdb_code, save_dir):\n",
    "    \"\"\"Downloads a PDB file in parallel.\"\"\"\n",
    "    pdb_file = PDB.PDBList()\n",
    "    pdb_file.retrieve_pdb_file(pdb_code, file_format=\"pdb\", pdir=save_dir, overwrite=False)\n",
    "\n",
    "def download_all_pdbs(pdb_codes, save_dir):\n",
    "    \"\"\"Downloads multiple PDB files in parallel.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_pdb, pdb_codes, [save_dir] * len(pdb_codes))\n",
    "\n",
    "\n",
    "# === 2️⃣ Amino Acid Conversion ===\n",
    "AA_DICT = {\n",
    "    \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "    \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "    \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "    \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\"\n",
    "}\n",
    "\n",
    "def three_to_one(resname):\n",
    "    \"\"\"Converts 3-letter residue name to 1-letter code.\"\"\"\n",
    "    return AA_DICT.get(resname, \"X\")  # 'X' for unknown residues\n",
    "\n",
    "\n",
    "# === 3️⃣ Extract FASTA from PDB ===\n",
    "def extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir):\n",
    "    \"\"\"Extracts the sequence of a specific chain from a PDB file and writes it as FASTA.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    fasta_filepath = os.path.join(fasta_dir, f\"{pdb_code}.fasta\")\n",
    "\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "\n",
    "    # Only process first model (speed optimization)\n",
    "    model = structure[0]\n",
    "    sequence, observed_residues = [], []\n",
    "\n",
    "    chain = model[chain_id] if chain_id in model else None\n",
    "    if chain:\n",
    "        for residue in chain.get_residues():\n",
    "            if PDB.is_aa(residue):\n",
    "                sequence.append(three_to_one(residue.get_resname()))\n",
    "                observed_residues.append(residue.id[1])\n",
    "\n",
    "    # Write FASTA\n",
    "    fasta_seq = SeqRecord(Seq(\"\".join(sequence)), id=f\"{pdb_code}_{chain_id}\", description=\"\")\n",
    "    SeqIO.write(fasta_seq, fasta_filepath, \"fasta\")\n",
    "\n",
    "    return observed_residues\n",
    "\n",
    "\n",
    "# === 4️⃣ Run DeepTMHMM Asynchronously ===\n",
    "def run_deeptmhmm(pdb_code, fasta_filepath, results_dir):\n",
    "    \"\"\"Runs DeepTMHMM for a single PDB code asynchronously.\"\"\"\n",
    "    pdb_results_dir = os.path.join(results_dir, pdb_code)\n",
    "    os.makedirs(pdb_results_dir, exist_ok=True)\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        [\"biolib\", \"run\", \"DTU/DeepTMHMM\", \"--fasta\", fasta_filepath],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        cwd=pdb_results_dir,\n",
    "        start_new_session=True  # Allows independent execution\n",
    "    )\n",
    "    return process\n",
    "\n",
    "\n",
    "def run_all_deeptmhmm(pdb_codes, fasta_dir, results_dir):\n",
    "    \"\"\"Runs DeepTMHMM for multiple PDB codes in parallel asynchronously.\"\"\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        processes = {\n",
    "            pdb_code: executor.submit(run_deeptmhmm, pdb_code, os.path.join(fasta_dir, f\"{pdb_code}.fasta\"), results_dir)\n",
    "            for pdb_code in pdb_codes\n",
    "        }\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    for pdb_code, future in processes.items():\n",
    "        process = future.result()\n",
    "        stdout, stderr = process.communicate()\n",
    "        print(f\"[{pdb_code}] STDOUT:\", stdout)\n",
    "        print(f\"[{pdb_code}] STDERR:\", stderr)\n",
    "\n",
    "\n",
    "def keep_only_tmr(results_dir, pdb_codes):\n",
    "    \"\"\"Removes all files except TMRs.gff3 in each PDB results folder.\"\"\"\n",
    "    for pdb_code in pdb_codes:\n",
    "        pdb_results_dir = os.path.join(results_dir, pdb_code, \"biolib_results\")  # Ensure correct subfolder\n",
    "\n",
    "        if os.path.exists(pdb_results_dir):\n",
    "            for filename in os.listdir(pdb_results_dir):\n",
    "                file_path = os.path.join(pdb_results_dir, filename)\n",
    "\n",
    "                if filename != \"TMRs.gff3\":\n",
    "                    try:\n",
    "                        os.remove(file_path)  # Remove only files, leave the folder\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not remove {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def extract_coordinates(pdb_code, pdb_dir, chain_id='A'):\n",
    "    \"\"\"Extracts coordinates from a specific chain in the PDB file.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "    \n",
    "    # Select the first model (as a default)\n",
    "    model = structure[0]\n",
    "    chain = model[chain_id]  # Get the chain\n",
    "\n",
    "    coordinates = []\n",
    "    for residue in chain:\n",
    "        if PDB.is_aa(residue):  # Ensure it's an amino acid\n",
    "            for atom in residue:\n",
    "                if atom.get_name() in ['N', 'CA', 'C', 'O']:  # Get backbone atoms only\n",
    "                    coordinates.append(atom.coord)\n",
    "                    \n",
    "    # Convert coordinates to numpy array and then PyTorch tensor\n",
    "    coord_array = np.array(coordinates)\n",
    "    L = sum(1 for residue in chain if PDB.is_aa(residue))  # Number of residues in chain\n",
    "    atoms = 4  # N, CA, C, O\n",
    "    xyz = 3  # x, y, z coordinates\n",
    "    \n",
    "    coord_tensor = torch.tensor(coord_array, dtype=torch.float32).reshape([L, atoms, xyz])\n",
    "    \n",
    "    return coord_tensor\n",
    "\n",
    "\n",
    "# === 5️⃣ TMH Extension Processing ===\n",
    "def calculate_desired_extensions(tmh_ranges, ss_data, max_extend=9):\n",
    "    \"\"\"Determine how much each TMH would like to extend based on consecutive 'H' residues.\"\"\"\n",
    "    desired_extensions = []\n",
    "\n",
    "    for start, end in tmh_ranges:\n",
    "        # Backward extension\n",
    "        backward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            if start - i >= 0 and ss_data[start - i] == \"H\":  # Check if index is within bounds\n",
    "                backward_extension += 1\n",
    "            else:\n",
    "                break  # Stop at first non-'H' or out-of-bounds\n",
    "\n",
    "        # Forward extension\n",
    "        forward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            if end + i < len(ss_data) and ss_data[end + i] == \"H\":  # Check if index is within bounds\n",
    "                forward_extension += 1\n",
    "            else:\n",
    "                break  # Stop at first non-'H' or out-of-bounds\n",
    "\n",
    "        desired_extensions.append((backward_extension, forward_extension))\n",
    "\n",
    "    return desired_extensions\n",
    "\n",
    "\n",
    "\n",
    "def calculate_available_spaces(tmh_ranges):\n",
    "    \"\"\"Calculate the number of residues available between consecutive TMHs.\"\"\"\n",
    "    available_spaces = []\n",
    "\n",
    "    for i in range(len(tmh_ranges) - 1):\n",
    "        prev_end = tmh_ranges[i][1]  # End of the current TMH\n",
    "        next_start = tmh_ranges[i + 1][0]  # Start of the next TMH\n",
    "        available_space = next_start - prev_end - 1  # Residues in between\n",
    "        available_spaces.append(available_space)\n",
    "\n",
    "    return available_spaces\n",
    "\n",
    "\n",
    "def reorder_gpcr_tmh_ends(tmh_extended_pairs):\n",
    "    \"\"\"Reorder TMH ends for a GPCR assuming 14 TMH ends in the given pattern.\"\"\"\n",
    "    pattern = [\"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\", \"intra\", \n",
    "               \"extra\", \"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\"]\n",
    "\n",
    "    reordered = []\n",
    "    for i, label in enumerate(pattern):\n",
    "        if label == \"extra\":\n",
    "            reordered.append(tmh_extended_pairs[i // 2][0])  # Take start residue\n",
    "        else:  # \"intra\"\n",
    "            reordered.append(tmh_extended_pairs[i // 2][1])  # Take end residue\n",
    "\n",
    "    return reordered\n",
    "\n",
    "\n",
    "# === 6️⃣ Define Directories and Execute ===\n",
    "def main():\n",
    "    pdb_codes = [\"6e67\", \"4gbr\", \"5d5a\", \"7dhi\"]\n",
    "    pdb_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\"\n",
    "    fasta_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\Fasta_files\"\n",
    "    results_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\DeepTMHMM_results\"\n",
    "\n",
    "    # Download PDB files in parallel\n",
    "    download_all_pdbs(pdb_codes, pdb_dir)\n",
    "\n",
    "    # Extract sequences\n",
    "    chain_id = \"A\"\n",
    "    all_observed_residues = {\n",
    "        pdb_code: extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir) for pdb_code in pdb_codes\n",
    "    }\n",
    "\n",
    "    # Run DeepTMHMM in parallel asynchronously\n",
    "    run_all_deeptmhmm(pdb_codes, fasta_dir, results_dir)\n",
    "    keep_only_tmr(results_dir, pdb_codes)\n",
    "\n",
    "    # Example DeepTMHMM result parsing and extension calculations\n",
    "    for pdb_code in pdb_codes:\n",
    "        tmh_ranges = []\n",
    "        tmh_result_file = os.path.join(results_dir, pdb_code, \"biolib_results\", \"TMRs.gff3\")\n",
    "\n",
    "        with open(tmh_result_file) as file:\n",
    "            for line in file:\n",
    "                if \"TMhelix\" in line:\n",
    "                    parts = line.strip().split(\"\\t\")\n",
    "                    start, end = int(parts[2]), int(parts[3])\n",
    "                    tmh_ranges.append((start, end))\n",
    "\n",
    "        # Use the correct pdb_filepath for extracting coordinates\n",
    "        pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "        coord_tensor = extract_coordinates(pdb_code, pdb_dir, chain_id)\n",
    "                    \n",
    "        # Get the secondary structure from DSSP using pydssp\n",
    "        ss_data = pydssp.assign(coord_tensor, out_type='c3')  # This will give you a list of \"H\" and \"-\" values\n",
    "\n",
    "        # Now pass this ss_data directly to calculate_desired_extensions\n",
    "        desired_extensions = calculate_desired_extensions(tmh_ranges, ss_data)\n",
    "\n",
    "        # Process available spaces\n",
    "        available_spaces = calculate_available_spaces(tmh_ranges)\n",
    "        max_extension = 9\n",
    "        desired_extensions = [(min(start, max_extension), min(end, max_extension)) for start, end in desired_extensions]\n",
    "\n",
    "        # Align extensions with available spaces\n",
    "        desired_extensions_dict = dict(enumerate(desired_extensions))\n",
    "        for i in range(len(available_spaces)):\n",
    "            total_desired = desired_extensions_dict[i][1] + desired_extensions_dict[i + 1][0]\n",
    "            available = available_spaces[i]\n",
    "\n",
    "            if total_desired > available:\n",
    "                half_space = math.floor(available / 2)\n",
    "                if half_space >= desired_extensions_dict[i][1]:\n",
    "                    desired_extensions_dict[i + 1] = (available - desired_extensions_dict[i][1], desired_extensions_dict[i + 1][1])\n",
    "                elif half_space >= desired_extensions_dict[i + 1][0]:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], available - desired_extensions_dict[i + 1][0])\n",
    "                else:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], half_space)\n",
    "                    desired_extensions_dict[i + 1] = (half_space, desired_extensions_dict[i + 1][1])\n",
    "\n",
    "        extended_tmh_ranges = []\n",
    "        for i, (start, end) in enumerate(tmh_ranges):\n",
    "            left_extension = desired_extensions_dict[i][0]\n",
    "            right_extension = desired_extensions_dict[i][1]\n",
    "            new_start = start - left_extension\n",
    "            new_end = end + right_extension\n",
    "            extended_tmh_ranges.append((new_start, new_end))\n",
    "\n",
    "        # Reorder TMH ends\n",
    "        tmh_extended_pairs = [\n",
    "            (all_observed_residues[pdb_code][start - 1], all_observed_residues[pdb_code][end - 1])\n",
    "            for start, end in extended_tmh_ranges\n",
    "        ]\n",
    "        reordered_tmh_ends = reorder_gpcr_tmh_ends(tmh_extended_pairs)\n",
    "        \n",
    "        print(f\"Reordered TMH ends for {pdb_code}: {reordered_tmh_ends}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49289df4-058e-4b57-b70b-6c070dacc36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
