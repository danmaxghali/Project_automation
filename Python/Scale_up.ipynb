{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6f6c9-4b97-4198-aeff-262efd32b23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2r4r.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2r4s.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2rh1.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3kj6.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3d4s.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3ny8.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3ny9.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3nya.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3p0g.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3sn6.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3pds.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4lde.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4gbr.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4qkx.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4ldl.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb4ldo.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5d6l.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5d5b.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6mxt.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6e67.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5x7d.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps1.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ni3.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps0.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb5d5a.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps4.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps3.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6oba.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6n48.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps2.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps5.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ps6.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb7dhi.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb7dhr.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb7bz2.ent' \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from Bio import PDB, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pydssp\n",
    "import math\n",
    "\n",
    "# Pick PDB structures\n",
    "pdb_codes = [\n",
    "    \n",
    "]\n",
    "\n",
    "# === 1ï¸âƒ£ Parallel PDB Downloading ===\n",
    "def download_pdb(pdb_code, save_dir):\n",
    "    \"\"\"Downloads a PDB file in parallel.\"\"\"\n",
    "    pdb_file = PDB.PDBList()\n",
    "    pdb_file.retrieve_pdb_file(pdb_code, file_format=\"pdb\", pdir=save_dir, overwrite=False)\n",
    "\n",
    "def download_all_pdbs(pdb_codes, save_dir):\n",
    "    \"\"\"Downloads multiple PDB files in parallel.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_pdb, pdb_codes, [save_dir] * len(pdb_codes))\n",
    "\n",
    "\n",
    "# === 2ï¸âƒ£ Amino Acid Conversion ===\n",
    "AA_DICT = {\n",
    "    \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "    \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "    \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "    \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\"\n",
    "}\n",
    "\n",
    "def three_to_one(resname):\n",
    "    \"\"\"Converts 3-letter residue name to 1-letter code.\"\"\"\n",
    "    return AA_DICT.get(resname, \"X\")  # 'X' for unknown residues\n",
    "\n",
    "\n",
    "# === 3ï¸âƒ£ Extract FASTA from PDB ===\n",
    "def extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir):\n",
    "    \"\"\"Extracts the sequence of a specific chain from a PDB file and writes it as FASTA.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    fasta_filepath = os.path.join(fasta_dir, f\"{pdb_code}.fasta\")\n",
    "\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "\n",
    "    # Only process first model (speed optimization)\n",
    "    model = structure[0]\n",
    "    sequence, observed_residues = [], []\n",
    "\n",
    "    chain = model[chain_id] if chain_id in model else None\n",
    "    if chain:\n",
    "        for residue in chain.get_residues():\n",
    "            if PDB.is_aa(residue):\n",
    "                sequence.append(three_to_one(residue.get_resname()))\n",
    "                observed_residues.append(residue.id[1])\n",
    "\n",
    "    # Write FASTA\n",
    "    fasta_seq = SeqRecord(Seq(\"\".join(sequence)), id=f\"{pdb_code}_{chain_id}\", description=\"\")\n",
    "    SeqIO.write(fasta_seq, fasta_filepath, \"fasta\")\n",
    "\n",
    "    return observed_residues\n",
    "\n",
    "def run_deeptmhmm(pdb_code, fasta_filepath_wsl):\n",
    "    pdb_results_dir = f\"/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Project_automation/Python/DeepTMHMM_results/{pdb_code}\"\n",
    "    os.makedirs(pdb_results_dir, exist_ok=True)\n",
    "\n",
    "    process = subprocess.run(\n",
    "        [\"wsl\", \"/home/dan/.local/bin/biolib\", \"run\", \"--local\", \"DTU/DeepTMHMM:1.0.24\", \"--fasta\", f\"{fasta_filepath_wsl}\"],\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        cwd=pdb_results_dir\n",
    "    )\n",
    "    \n",
    "    # print(process.stdout)\n",
    "    # print(process.stderr)\n",
    "    \n",
    "    return pdb_code\n",
    "\n",
    "def keep_only_tmr(results_dir, pdb_codes):\n",
    "    \"\"\"Removes all files except TMRs.gff3 in each PDB results folder.\"\"\"\n",
    "    for pdb_code in pdb_codes:\n",
    "        pdb_results_dir = os.path.join(results_dir, pdb_code, \"biolib_results\")  # Ensure correct subfolder\n",
    "\n",
    "        if os.path.exists(pdb_results_dir):\n",
    "            for filename in os.listdir(pdb_results_dir):\n",
    "                file_path = os.path.join(pdb_results_dir, filename)\n",
    "\n",
    "                if filename != \"TMRs.gff3\":\n",
    "                    try:\n",
    "                        os.remove(file_path)  # Remove only files, leave the folder\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not remove {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def extract_coordinates(pdb_code, pdb_dir, chain_id='A'):\n",
    "    \"\"\"Extracts coordinates from a specific chain in the PDB file.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "    \n",
    "    # Select the first model (as a default)\n",
    "    model = structure[0]\n",
    "    chain = model[chain_id]  # Get the chain\n",
    "\n",
    "    coordinates = []\n",
    "    for residue in chain:\n",
    "        if PDB.is_aa(residue):  # Ensure it's an amino acid\n",
    "            for atom in residue:\n",
    "                if atom.get_name() in ['N', 'CA', 'C', 'O']:  # Get backbone atoms only\n",
    "                    coordinates.append(atom.coord)\n",
    "                    \n",
    "    # Convert coordinates to numpy array and then PyTorch tensor\n",
    "    coord_array = np.array(coordinates)\n",
    "    L = sum(1 for residue in chain if PDB.is_aa(residue))  # Number of residues in chain\n",
    "    atoms = 4  # N, CA, C, O\n",
    "    xyz = 3  # x, y, z coordinates\n",
    "    \n",
    "    coord_tensor = torch.tensor(coord_array, dtype=torch.float32).reshape([L, atoms, xyz])\n",
    "    \n",
    "    return coord_tensor\n",
    "\n",
    "\n",
    "# === 5ï¸âƒ£ TMH Extension Processing ===\n",
    "def calculate_desired_extensions(tmh_ranges, ss_data, max_extend=9):\n",
    "    \"\"\"Determine how much each TMH would like to extend based on consecutive 'H' residues.\"\"\"\n",
    "    desired_extensions = []\n",
    "\n",
    "    for start, end in tmh_ranges:\n",
    "        if np.isnan(start) or np.isnan(end):\n",
    "            # Skip if start or end are NaN (missing)\n",
    "            desired_extensions.append((0, 0))\n",
    "            continue\n",
    "        \n",
    "        # Backward extension\n",
    "        backward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            ss_index = start - i  # DSSP index is offset by 1 (residue indices are 1-based)\n",
    "            if ss_index >= 0 and ss_data[ss_index] == \"H\":\n",
    "                backward_extension += 1\n",
    "            else:\n",
    "                break  # Stop at first non-'H' or out-of-bounds\n",
    "\n",
    "        # Forward extension\n",
    "        forward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            ss_index = end + i  # DSSP index is offset by 1 (residue indices are 1-based)\n",
    "            if ss_index < len(ss_data) and ss_data[ss_index] == \"H\":\n",
    "                forward_extension += 1\n",
    "            else:\n",
    "                break  # Stop at first non-'H' or out-of-bounds\n",
    "\n",
    "        desired_extensions.append((backward_extension, forward_extension))\n",
    "\n",
    "    return desired_extensions\n",
    "    \n",
    "def calculate_available_spaces(tmh_ranges):\n",
    "    \"\"\"Calculate the number of residues available between consecutive TMHs.\"\"\"\n",
    "    available_spaces = []\n",
    "\n",
    "    for i in range(len(tmh_ranges) - 1):\n",
    "        prev_end = tmh_ranges[i][1]  # End of the current TMH\n",
    "        next_start = tmh_ranges[i + 1][0]  # Start of the next TMH\n",
    "        available_space = next_start - prev_end - 1  # Residues in between\n",
    "        available_spaces.append(available_space)\n",
    "\n",
    "    return available_spaces\n",
    "\n",
    "\n",
    "def reorder_gpcr_tmh_ends(tmh_extended_pairs):\n",
    "    \"\"\"Reorder TMH ends for a GPCR assuming 14 TMH ends in the given pattern.\"\"\"\n",
    "    pattern = [\"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\", \"intra\", \n",
    "               \"extra\", \"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\"]\n",
    "\n",
    "    reordered = []\n",
    "    for i, label in enumerate(pattern):\n",
    "        if label == \"extra\":\n",
    "            reordered.append(tmh_extended_pairs[i // 2][0])  # Take start residue\n",
    "        else:  # \"intra\"\n",
    "            reordered.append(tmh_extended_pairs[i // 2][1])  # Take end residue\n",
    "\n",
    "    return reordered\n",
    "\n",
    "def main():\n",
    "    pdb_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\"\n",
    "    fasta_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\Fasta_files\"\n",
    "    results_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\DeepTMHMM_results\"\n",
    "    \n",
    "    # Download PDB files\n",
    "    download_all_pdbs(pdb_codes, pdb_dir)\n",
    "\n",
    "    # Extract sequences\n",
    "    chain_id = \"A\"\n",
    "    all_observed_residues = {\n",
    "        pdb_code: extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir) for pdb_code in pdb_codes\n",
    "    }\n",
    "\n",
    "    # Run DeepTMHMM in parallel asynchronously\n",
    "    for pdb_code in pdb_codes:\n",
    "        fasta_filepath_wsl = f\"/mnt/c/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Project_automation/Python/Fasta_files/{pdb_code}.fasta\"  \n",
    "        run_deeptmhmm(pdb_code, fasta_filepath_wsl)\n",
    "    keep_only_tmr(results_dir, pdb_codes)\n",
    "\n",
    "    pdb_data = {}\n",
    "\n",
    "    for pdb_code in pdb_codes:\n",
    "        tmh_ranges = []\n",
    "        tmh_result_file = os.path.join(results_dir, pdb_code, \"biolib_results\", \"TMRs.gff3\")\n",
    "\n",
    "        if os.path.exists(tmh_result_file):\n",
    "            with open(tmh_result_file) as file:\n",
    "                for line in file:\n",
    "                    if \"TMhelix\" in line:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        start, end = int(parts[2]), int(parts[3])\n",
    "                        tmh_ranges.append((start, end))\n",
    "\n",
    "        if len(tmh_ranges) < 7:\n",
    "            print(f\"âš ï¸ Warning: {pdb_code} has only {len(tmh_ranges)} TMHs detected.\")\n",
    "            tmh_ranges += [(np.nan, np.nan)] * (7 - len(tmh_ranges))\n",
    "\n",
    "        # Extract secondary structure\n",
    "        pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "        coord_tensor = extract_coordinates(pdb_code, pdb_dir, chain_id)\n",
    "        ss_data = pydssp.assign(coord_tensor, out_type='c3')\n",
    "\n",
    "        # Compute desired extensions\n",
    "        desired_extensions = calculate_desired_extensions(tmh_ranges, ss_data)\n",
    "        available_spaces = calculate_available_spaces(tmh_ranges)\n",
    "        max_extension = 9\n",
    "\n",
    "        desired_extensions = [\n",
    "            (min(start, max_extension), min(end, max_extension)) for start, end in desired_extensions\n",
    "        ]\n",
    "\n",
    "        # Align extensions with available spaces\n",
    "        desired_extensions_dict = dict(enumerate(desired_extensions))\n",
    "        for i in range(len(available_spaces)):\n",
    "            total_desired = desired_extensions_dict[i][1] + desired_extensions_dict[i + 1][0]\n",
    "            available = available_spaces[i]\n",
    "\n",
    "            if total_desired > available:\n",
    "                half_space = math.floor(available / 2)\n",
    "                if half_space >= desired_extensions_dict[i][1]:\n",
    "                    desired_extensions_dict[i + 1] = (available - desired_extensions_dict[i][1], desired_extensions_dict[i + 1][1])\n",
    "                elif half_space >= desired_extensions_dict[i + 1][0]:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], available - desired_extensions_dict[i + 1][0])\n",
    "                else:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], half_space)\n",
    "                    desired_extensions_dict[i + 1] = (half_space, desired_extensions_dict[i + 1][1])\n",
    "\n",
    "        extended_tmh_ranges = []\n",
    "        for i, (start, end) in enumerate(tmh_ranges):\n",
    "            left_extension = desired_extensions_dict[i][0]\n",
    "            right_extension = desired_extensions_dict[i][1]\n",
    "\n",
    "            if np.isnan(start) or np.isnan(end):\n",
    "                new_start, new_end = np.nan, np.nan  \n",
    "            else:\n",
    "                new_start = start - left_extension\n",
    "                new_end = end + right_extension\n",
    "\n",
    "            extended_tmh_ranges.append((new_start, new_end))\n",
    "\n",
    "        # Reorder TMH ends\n",
    "        tmh_extended_pairs = [\n",
    "            (all_observed_residues[pdb_code][int(start) - 1] if not np.isnan(start) else np.nan,\n",
    "             all_observed_residues[pdb_code][int(end) - 1] if not np.isnan(end) else np.nan)\n",
    "            for start, end in extended_tmh_ranges\n",
    "        ]\n",
    "\n",
    "        reordered_tmh_ends = reorder_gpcr_tmh_ends(tmh_extended_pairs)\n",
    "        print(f\"Reordered TMH ends for {pdb_code}: {reordered_tmh_ends}\")\n",
    "\n",
    "        # Store reordered_tmh_ends in pdb_data\n",
    "        pdb_data[pdb_code] = {\"A\": reordered_tmh_ends}\n",
    "\n",
    "    print(pdb_data)  # Debugging step\n",
    "    return pdb_data  # Return final dictionary\n",
    "\n",
    "# ðŸ”¹ Run main() and store result in a variable\n",
    "if __name__ == \"__main__\":\n",
    "    pdb_data = main()\n",
    "    print(\"Final PDB Data:\", pdb_data)  # Use or return it outside of main\n",
    "\n",
    "\n",
    "\n",
    "# Create function to extract coordinates\n",
    "def get_coords(pdb_id, chain_id, residues):\n",
    "    pdb_id = pdb_id.lower()\n",
    "    pdbl = PDB.PDBList()\n",
    "    coords = [pdb_id.upper(), chain_id] \n",
    "\n",
    "    # Create temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        pdb_file_path = pdbl.retrieve_pdb_file(pdb_id, pdir=temp_dir, file_format=\"pdb\")\n",
    "\n",
    "        # Parse the file\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(pdb_id, pdb_file_path)\n",
    "\n",
    "        # Extract resolution\n",
    "        resolution = \"Unknown\"\n",
    "        with open(pdb_file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"REMARK   2 RESOLUTION\"):\n",
    "                    resolution = line.split()[3]\n",
    "                    break\n",
    "\n",
    "        coords.insert(1, resolution) \n",
    "        \n",
    "        # Loop through file to extract residue coordinates\n",
    "        for residue_id in residues:\n",
    "            found = False\n",
    "            for model in structure:\n",
    "                if chain_id in model:\n",
    "                    chain = model[chain_id]\n",
    "                    if residue_id in chain:\n",
    "                        residue = chain[residue_id]\n",
    "                        residue_name = residue.get_resname()\n",
    "                        for atom in residue:\n",
    "                            if atom.get_name() == \"CA\":\n",
    "                                coords.extend([f\"{residue_name}{residue_id}\", *atom.coord])\n",
    "                                found = True\n",
    "                                break\n",
    "            if not found:\n",
    "                coords.extend([f\"Unknown{residue_id}\", \"NA\", \"NA\", \"NA\"])\n",
    "\n",
    "    return coords\n",
    "\n",
    "data = []\n",
    "\n",
    "# Collect and prepare items to be processed by function\n",
    "for pdb_id, chains in pdb_data.items():\n",
    "    for chain_id, residues in chains.items():\n",
    "        data.append(get_coords(pdb_id, chain_id, residues))\n",
    "\n",
    "\n",
    "if not pdb_data or all(not chains for chains in pdb_data.values()):\n",
    "    raise ValueError(\"pdb_data is empty or contains no residue information.\")\n",
    "\n",
    "max_residues = max(len(residues) for chains in pdb_data.values() for residues in chains.values())\n",
    "# Organise the data frame for accurate conversion to Excel\n",
    "max_residues = max(len(residues) for chains in pdb_data.values() for residues in chains.values())\n",
    "\n",
    "# Define columns dynamically\n",
    "columns = [\"PDB ID\", \"Resolution\", \"Chain\"] + sum([[\"Res\", \"X\", \"Y\", \"Z\"]] * max_residues, [])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save as Excel output\n",
    "output_file = \"C:/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Landmarks/Automated landmarks/Protein_coordinates.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(pdb_data)\n",
    "\n",
    "#Print coordinates as a test\n",
    "coordinates = get_coords(pdb_id, chain_id, residues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8b023-f11f-4051-b0b6-759370b950c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
