{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c147db9-e072-4f2d-a9fa-4946b95719ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3sn6.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb6ni3.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb7bz2.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb7dhi.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb7dhr.ent' \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PDBParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 314\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Run main() and store result in a variable\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 314\u001b[0m     pdb_data \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal PDB Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pdb_data)  \u001b[38;5;66;03m# Use or return it outside of main\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# # Create function to extract coordinates\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# def get_coords(pdb_id, chain_id, residues):\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m#     pdb_id = pdb_id.lower()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# #Print coordinates as a test\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# coordinates = get_coords(pdb_id, chain_id, residues)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 216\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    212\u001b[0m download_all_pdbs(pdb_codes, pdb_dir)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Extract sequences\u001b[39;00m\n\u001b[0;32m    215\u001b[0m all_observed_residues \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 216\u001b[0m     pdb_code: \u001b[43mextract_pdb_fasta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdb_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdb_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfasta_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pdb_code \u001b[38;5;129;01min\u001b[39;00m pdb_codes\n\u001b[0;32m    217\u001b[0m }\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Run DeepTMHMM in parallel asynchronously\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdb_code \u001b[38;5;129;01min\u001b[39;00m pdb_codes:\n",
      "Cell \u001b[1;32mIn[9], line 61\u001b[0m, in \u001b[0;36mextract_pdb_fasta\u001b[1;34m(pdb_code, pdb_dir, fasta_dir)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extracts sequences of all chains from a PDB file and writes them as FASTA.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m pdb_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdb_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdb\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdb_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mPDBParser\u001b[49m(QUIET\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     62\u001b[0m structure \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mget_structure(pdb_code, pdb_filepath)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Only process first model (speed optimization)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PDBParser' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from Bio import PDB, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pydssp\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Pick PDB structures\n",
    "pdb_codes = [\n",
    "    \"3sn6\",\n",
    "    \"6ni3\",\n",
    "    \"7bz2\",\n",
    "    \"7dhi\",\n",
    "    \"7dhr\"\n",
    "    # \"8dcs\",\n",
    "    # \"7jjo\",\n",
    "    # \"8dcr\",\n",
    "    # \"7s0f\",\n",
    "    # \"2ycw\",\n",
    "    # \"2y04\"\n",
    "]\n",
    "\n",
    "# === 1ï¸âƒ£ Parallel PDB Downloading ===\n",
    "def download_pdb(pdb_code, save_dir):\n",
    "    \"\"\"Downloads a PDB file in parallel.\"\"\"\n",
    "    pdb_file = PDB.PDBList()\n",
    "    pdb_file.retrieve_pdb_file(pdb_code, file_format=\"pdb\", pdir=save_dir, overwrite=False)\n",
    "\n",
    "def download_all_pdbs(pdb_codes, save_dir):\n",
    "    \"\"\"Downloads multiple PDB files in parallel.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_pdb, pdb_codes, [save_dir] * len(pdb_codes))\n",
    "\n",
    "\n",
    "# === 2ï¸âƒ£ Amino Acid Conversion ===\n",
    "AA_DICT = {\n",
    "    \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "    \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "    \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "    \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\"\n",
    "}\n",
    "\n",
    "def three_to_one(resname):\n",
    "    \"\"\"Converts 3-letter residue name to 1-letter code.\"\"\"\n",
    "    return AA_DICT.get(resname, \"X\")  # 'X' for unknown residues\n",
    "\n",
    "\n",
    "# === 3ï¸âƒ£ Extract FASTA from PDB ===\n",
    "def extract_pdb_fasta(pdb_code, pdb_dir, fasta_dir):\n",
    "    \"\"\"Extracts sequences of all chains from a PDB file and writes them as FASTA.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "\n",
    "    # Only process first model (speed optimization)\n",
    "    model = structure[0]\n",
    "    chain_sequences = {}\n",
    "\n",
    "    for chain in model:\n",
    "        chain_id = chain.id\n",
    "        sequence, observed_residues = [], []\n",
    "\n",
    "        for residue in chain.get_residues():\n",
    "            if is_aa(residue):\n",
    "                sequence.append(three_to_one(residue.get_resname()))\n",
    "                observed_residues.append(residue.id[1])\n",
    "\n",
    "        if sequence:  # Ensure the chain contains amino acids\n",
    "            fasta_seq = SeqRecord(Seq(\"\".join(sequence)), id=f\"{pdb_code}_{chain_id}\", description=\"\")\n",
    "            fasta_filepath = os.path.join(fasta_dir, f\"{pdb_code}_{chain_id}.fasta\")\n",
    "            SeqIO.write(fasta_seq, fasta_filepath, \"fasta\")\n",
    "            chain_sequences[chain_id] = observed_residues\n",
    "\n",
    "    return chain_sequences  # Dictionary of observed residues for each chain\n",
    "\n",
    "\n",
    "def run_deeptmhmm(pdb_code, fasta_filepath_wsl):\n",
    "    pdb_results_dir = f\"/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Project_automation/Python/DeepTMHMM_results/{pdb_code}\"\n",
    "    os.makedirs(pdb_results_dir, exist_ok=True)\n",
    "\n",
    "    process = subprocess.run(\n",
    "        [\"wsl\", \"/home/dan/.local/bin/biolib\", \"run\", \"--local\", \"DTU/DeepTMHMM:1.0.24\", \"--fasta\", f\"{fasta_filepath_wsl}\"],\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        cwd=pdb_results_dir\n",
    "    )\n",
    "    \n",
    "    # print(process.stdout)\n",
    "    # print(process.stderr)\n",
    "    \n",
    "    return pdb_code\n",
    "\n",
    "def keep_only_tmr(results_dir, pdb_codes):\n",
    "    \"\"\"Removes all files except TMRs.gff3 in each PDB results folder.\"\"\"\n",
    "    for pdb_code in pdb_codes:\n",
    "        pdb_results_dir = os.path.join(results_dir, pdb_code, \"biolib_results\")  # Ensure correct subfolder\n",
    "\n",
    "        if os.path.exists(pdb_results_dir):\n",
    "            for filename in os.listdir(pdb_results_dir):\n",
    "                file_path = os.path.join(pdb_results_dir, filename)\n",
    "\n",
    "                if filename != \"TMRs.gff3\":\n",
    "                    try:\n",
    "                        os.remove(file_path)  # Remove only files, leave the folder\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not remove {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def extract_coordinates(pdb_code, pdb_dir, chain_id):\n",
    "    \"\"\"Extracts coordinates from a specific chain in the PDB file.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "    \n",
    "    # Select the first model (as a default)\n",
    "    model = structure[0]\n",
    "    chain = model[chain_id]  # Get the chain\n",
    "\n",
    "    coordinates = []\n",
    "    for residue in chain:\n",
    "        if PDB.is_aa(residue):  # Ensure it's an amino acid\n",
    "            for atom in residue:\n",
    "                if atom.get_name() in ['N', 'CA', 'C', 'O']:  # Get backbone atoms only\n",
    "                    coordinates.append(atom.coord)\n",
    "                    \n",
    "    # Convert coordinates to numpy array and then PyTorch tensor\n",
    "    coord_array = np.array(coordinates)\n",
    "    L = sum(1 for residue in chain if PDB.is_aa(residue))  # Number of residues in chain\n",
    "    atoms = 4  # N, CA, C, O\n",
    "    xyz = 3  # x, y, z coordinates\n",
    "    \n",
    "    coord_tensor = torch.tensor(coord_array, dtype=torch.float32).reshape([L, atoms, xyz])\n",
    "    \n",
    "    return coord_tensor\n",
    "\n",
    "\n",
    "# === 5ï¸âƒ£ TMH Extension Processing ===\n",
    "def calculate_desired_extensions(tmh_ranges, ss_data, max_extend=9):\n",
    "    \"\"\"Determine how much each TMH would like to extend based on consecutive 'H' residues.\"\"\"\n",
    "    desired_extensions = []\n",
    "\n",
    "    for start, end in tmh_ranges:\n",
    "        if np.isnan(start) or np.isnan(end):\n",
    "            # Skip if start or end are NaN (missing)\n",
    "            desired_extensions.append((0, 0))\n",
    "            continue\n",
    "        \n",
    "        # Backward extension\n",
    "        backward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            ss_index = start - i  # DSSP index is offset by 1 (residue indices are 1-based)\n",
    "            if ss_index >= 0 and ss_data[ss_index] == \"H\":\n",
    "                backward_extension += 1\n",
    "            else:\n",
    "                break  # Stop at first non-'H' or out-of-bounds\n",
    "\n",
    "        # Forward extension\n",
    "        forward_extension = 0\n",
    "        for i in range(1, max_extend + 1):\n",
    "            ss_index = end + i  # DSSP index is offset by 1 (residue indices are 1-based)\n",
    "            if ss_index < len(ss_data) and ss_data[ss_index] == \"H\":\n",
    "                forward_extension += 1\n",
    "            else:\n",
    "                break  # Stop at first non-'H' or out-of-bounds\n",
    "\n",
    "        desired_extensions.append((backward_extension, forward_extension))\n",
    "\n",
    "    return desired_extensions\n",
    "    \n",
    "def calculate_available_spaces(tmh_ranges):\n",
    "    \"\"\"Calculate the number of residues available between consecutive TMHs.\"\"\"\n",
    "    available_spaces = []\n",
    "\n",
    "    for i in range(len(tmh_ranges) - 1):\n",
    "        prev_end = tmh_ranges[i][1]  # End of the current TMH\n",
    "        next_start = tmh_ranges[i + 1][0]  # Start of the next TMH\n",
    "        available_space = next_start - prev_end - 1  # Residues in between\n",
    "        available_spaces.append(available_space)\n",
    "\n",
    "    return available_spaces\n",
    "\n",
    "\n",
    "def reorder_gpcr_tmh_ends(tmh_extended_pairs):\n",
    "    \"\"\"Reorder TMH ends for a GPCR assuming 14 TMH ends in the given pattern.\"\"\"\n",
    "    pattern = [\"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\", \"intra\", \n",
    "               \"extra\", \"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\"]\n",
    "\n",
    "    reordered = []\n",
    "    for i, label in enumerate(pattern):\n",
    "        if label == \"extra\":\n",
    "            reordered.append(tmh_extended_pairs[i // 2][0])  # Take start residue\n",
    "        else:  # \"intra\"\n",
    "            reordered.append(tmh_extended_pairs[i // 2][1])  # Take end residue\n",
    "\n",
    "    return reordered\n",
    "\n",
    "def main():\n",
    "    pdb_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\"\n",
    "    fasta_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\Fasta_files\"\n",
    "    results_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\DeepTMHMM_results\"\n",
    "    \n",
    "    # Download PDB files\n",
    "    download_all_pdbs(pdb_codes, pdb_dir)\n",
    "\n",
    "    # Extract sequences\n",
    "    all_observed_residues = {\n",
    "        pdb_code: extract_pdb_fasta(pdb_code, pdb_dir, fasta_dir) for pdb_code in pdb_codes\n",
    "    }\n",
    "\n",
    "    # Run DeepTMHMM in parallel asynchronously\n",
    "    for pdb_code in pdb_codes:\n",
    "        fasta_filepath_wsl = f\"/mnt/c/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Project_automation/Python/Fasta_files/{pdb_code}.fasta\"  \n",
    "        # run_deeptmhmm(pdb_code, fasta_filepath_wsl)\n",
    "    keep_only_tmr(results_dir, pdb_codes)\n",
    "\n",
    "    pdb_data = {}\n",
    "\n",
    "    for pdb_code in pdb_codes:\n",
    "        tmh_ranges = []\n",
    "        tmh_result_file = os.path.join(results_dir, pdb_code, \"biolib_results\", \"TMRs.gff3\")\n",
    "\n",
    "        chain_id = None\n",
    "\n",
    "        if os.path.exists(tmh_result_file):\n",
    "            with open(tmh_result_file) as file:\n",
    "                for line in file:\n",
    "                    if \"TMhelix\" in line:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        start, end = int(parts[2]), int(parts[3])\n",
    "                        tmh_ranges.append((start, end))\n",
    "                    match = re.search(rf\"{pdb_code}_([A-Z])\", line)\n",
    "                    if match and chain_id is None:\n",
    "                        chain_id = match.group(1)\n",
    "                        break\n",
    "        if chain_id is None:\n",
    "                    print(f\"âš ï¸ Warning: No chain ID found for {pdb_code}.\")\n",
    "                    continue  # Skip to next PDB if chain is missing\n",
    "        \n",
    "        if len(tmh_ranges) < 7:\n",
    "            print(f\"âš ï¸ Warning: {pdb_code} has only {len(tmh_ranges)} TMHs detected\")\n",
    "            tmh_ranges += [(np.nan, np.nan)] * (7 - len(tmh_ranges))\n",
    "\n",
    "        # Extract secondary structure\n",
    "        pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "        coord_tensor = extract_coordinates(pdb_code, pdb_dir, chain_id)\n",
    "        ss_data = pydssp.assign(coord_tensor, out_type='c3')\n",
    "\n",
    "        # Compute desired extensions\n",
    "        desired_extensions = calculate_desired_extensions(tmh_ranges, ss_data)\n",
    "        available_spaces = calculate_available_spaces(tmh_ranges)\n",
    "        max_extension = 9\n",
    "\n",
    "        desired_extensions = [\n",
    "            (min(start, max_extension), min(end, max_extension)) for start, end in desired_extensions\n",
    "        ]\n",
    "\n",
    "        # Align extensions with available spaces\n",
    "        desired_extensions_dict = dict(enumerate(desired_extensions))\n",
    "        for i in range(len(available_spaces)):\n",
    "            total_desired = desired_extensions_dict[i][1] + desired_extensions_dict[i + 1][0]\n",
    "            available = available_spaces[i]\n",
    "\n",
    "            if total_desired > available:\n",
    "                half_space = math.floor(available / 2)\n",
    "                if half_space >= desired_extensions_dict[i][1]:\n",
    "                    desired_extensions_dict[i + 1] = (available - desired_extensions_dict[i][1], desired_extensions_dict[i + 1][1])\n",
    "                elif half_space >= desired_extensions_dict[i + 1][0]:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], available - desired_extensions_dict[i + 1][0])\n",
    "                else:\n",
    "                    desired_extensions_dict[i] = (desired_extensions_dict[i][0], half_space)\n",
    "                    desired_extensions_dict[i + 1] = (half_space, desired_extensions_dict[i + 1][1])\n",
    "\n",
    "        extended_tmh_ranges = []\n",
    "        for i, (start, end) in enumerate(tmh_ranges):\n",
    "            left_extension = desired_extensions_dict[i][0]\n",
    "            right_extension = desired_extensions_dict[i][1]\n",
    "\n",
    "            if np.isnan(start) or np.isnan(end):\n",
    "                new_start, new_end = np.nan, np.nan  \n",
    "            else:\n",
    "                new_start = start - left_extension\n",
    "                new_end = end + right_extension\n",
    "\n",
    "            extended_tmh_ranges.append((new_start, new_end))\n",
    "\n",
    "\n",
    "        # Reorder TMH ends\n",
    "        tmh_extended_pairs = [\n",
    "            (all_observed_residues[pdb_code][int(start) - 1] if not np.isnan(start) else np.nan,\n",
    "             all_observed_residues[pdb_code][int(end) - 1] if not np.isnan(end) else np.nan)\n",
    "            for start, end in extended_tmh_ranges\n",
    "        ]\n",
    "\n",
    "        reordered_tmh_ends = reorder_gpcr_tmh_ends(tmh_extended_pairs)\n",
    "        print(f\"Reordered TMH ends for {pdb_code}: {reordered_tmh_ends}\")\n",
    "\n",
    "        # Store reordered_tmh_ends in pdb_data\n",
    "        pdb_data[pdb_code] = {chain_id: reordered_tmh_ends}\n",
    "\n",
    "    print(pdb_data)  # Debugging step\n",
    "    return pdb_data  # Return final dictionary\n",
    "\n",
    "# ðŸ”¹ Run main() and store result in a variable\n",
    "if __name__ == \"__main__\":\n",
    "    pdb_data = main()\n",
    "    print(\"Final PDB Data:\", pdb_data)  # Use or return it outside of main\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Create function to extract coordinates\n",
    "# def get_coords(pdb_id, chain_id, residues):\n",
    "#     pdb_id = pdb_id.lower()\n",
    "#     pdbl = PDB.PDBList()\n",
    "#     coords = [pdb_id.upper(), chain_id] \n",
    "\n",
    "#     # Create temporary directory\n",
    "#     with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#         pdb_file_path = pdbl.retrieve_pdb_file(pdb_id, pdir=temp_dir, file_format=\"pdb\")\n",
    "\n",
    "#         # Parse the file\n",
    "#         parser = PDB.PDBParser(QUIET=True)\n",
    "#         structure = parser.get_structure(pdb_id, pdb_file_path)\n",
    "\n",
    "#         # Extract resolution\n",
    "#         resolution = \"Unknown\"\n",
    "#         with open(pdb_file_path, \"r\") as f:\n",
    "#             for line in f:\n",
    "#                 if line.startswith(\"REMARK   2 RESOLUTION\"):\n",
    "#                     resolution = line.split()[3]\n",
    "#                     break\n",
    "\n",
    "#         coords.insert(1, resolution) \n",
    "        \n",
    "#         # Loop through file to extract residue coordinates\n",
    "#         for residue_id in residues:\n",
    "#             found = False\n",
    "#             for model in structure:\n",
    "#                 if chain_id in model:\n",
    "#                     chain = model[chain_id]\n",
    "#                     if residue_id in chain:\n",
    "#                         residue = chain[residue_id]\n",
    "#                         residue_name = residue.get_resname()\n",
    "#                         for atom in residue:\n",
    "#                             if atom.get_name() == \"CA\":\n",
    "#                                 coords.extend([f\"{residue_name}{residue_id}\", *atom.coord])\n",
    "#                                 found = True\n",
    "#                                 break\n",
    "#             if not found:\n",
    "#                 coords.extend([f\"Unknown{residue_id}\", \"NA\", \"NA\", \"NA\"])\n",
    "\n",
    "\n",
    "#     return coords\n",
    "\n",
    "# data = []\n",
    "\n",
    "# # Collect and prepare items to be processed by function\n",
    "# for pdb_id, chains in pdb_data.items():\n",
    "#     for chain_id, residues in chains.items():\n",
    "#         data.append(get_coords(pdb_id, chain_id, residues))\n",
    "\n",
    "\n",
    "# if not pdb_data or all(not chains for chains in pdb_data.values()):\n",
    "#     raise ValueError(\"pdb_data is empty or contains no residue information.\")\n",
    "\n",
    "# max_residues = max(len(residues) for chains in pdb_data.values() for residues in chains.values())\n",
    "# # Organise the data frame for accurate conversion to Excel\n",
    "# max_residues = max(len(residues) for chains in pdb_data.values() for residues in chains.values())\n",
    "\n",
    "# # Define columns dynamically\n",
    "# columns = [\"PDB ID\", \"Resolution\", \"Chain\"] + sum([[\"Res\", \"X\", \"Y\", \"Z\"]] * max_residues, [])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# # Save as Excel output\n",
    "# output_file = \"C:/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Landmarks/Automated landmarks/Protein_coordinates.xlsx\"\n",
    "# df.to_excel(output_file, index=False)\n",
    "\n",
    "# print(pdb_data)\n",
    "\n",
    "# #Print coordinates as a test\n",
    "# coordinates = get_coords(pdb_id, chain_id, residues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0321f6af-7f02-45f5-a8d0-42c591b70358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDB structure '3sn6'...\n",
      "{'3sn6': {'R': [31, 40, 59, 42, 66, 95, 135, 102, 146, 171, 229, 196, 266, 298]}}\n",
      "Downloading PDB structure '3sn6'...\n"
     ]
    }
   ],
   "source": [
    "# Create function to extract coordinates\n",
    "def get_coords(pdb_id, chain_id, residues):\n",
    "    pdb_id = pdb_id.lower()\n",
    "    pdbl = PDB.PDBList()\n",
    "    coords = [pdb_id.upper(), chain_id] \n",
    "\n",
    "    # Create temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        pdb_file_path = pdbl.retrieve_pdb_file(pdb_id, pdir=temp_dir, file_format=\"pdb\")\n",
    "\n",
    "        # Parse the file\n",
    "        parser = PDB.PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(pdb_id, pdb_file_path)\n",
    "\n",
    "        # Extract resolution\n",
    "        resolution = \"Unknown\"\n",
    "        with open(pdb_file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"REMARK   2 RESOLUTION\"):\n",
    "                    resolution = line.split()[3]\n",
    "                    break\n",
    "\n",
    "        coords.insert(1, resolution) \n",
    "        \n",
    "        # Loop through file to extract residue coordinates\n",
    "        for residue_id in residues:\n",
    "            found = False\n",
    "            for model in structure:\n",
    "                if chain_id in model:\n",
    "                    chain = model[chain_id]\n",
    "                    if residue_id in chain:\n",
    "                        residue = chain[residue_id]\n",
    "                        residue_name = residue.get_resname()\n",
    "                        for atom in residue:\n",
    "                            if atom.get_name() == \"CA\":\n",
    "                                coords.extend([f\"{residue_name}{residue_id}\", *atom.coord])\n",
    "                                found = True\n",
    "                                break\n",
    "            if not found:\n",
    "                coords.extend([f\"Unknown{residue_id}\", \"NA\", \"NA\", \"NA\"])\n",
    "\n",
    "    return coords\n",
    "\n",
    "data = []\n",
    "\n",
    "# Collect and prepare items to be processed by function\n",
    "for pdb_id, chains in pdb_data.items():\n",
    "    for chain_id, residues in chains.items():\n",
    "        data.append(get_coords(pdb_id, chain_id, residues))\n",
    "\n",
    "\n",
    "if not pdb_data or all(not chains for chains in pdb_data.values()):\n",
    "    raise ValueError(\"pdb_data is empty or contains no residue information.\")\n",
    "\n",
    "max_residues = max(len(residues) for chains in pdb_data.values() for residues in chains.values())\n",
    "# Organise the data frame for accurate conversion to Excel\n",
    "max_residues = max(len(residues) for chains in pdb_data.values() for residues in chains.values())\n",
    "\n",
    "# Define columns dynamically\n",
    "columns = [\"PDB ID\", \"Resolution\", \"Chain\"] + sum([[\"Res\", \"X\", \"Y\", \"Z\"]] * max_residues, [])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save as Excel output\n",
    "output_file = \"C:/Users/Student/OneDrive - Aston University/Documents/Biology/Project/Landmarks/Automated landmarks/Protein_coordinates.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(pdb_data)\n",
    "\n",
    "#Print coordinates as a test\n",
    "coordinates = get_coords(pdb_id, chain_id, residues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ea684e-97b5-4e1c-8b62-cf7b48025b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008,\n",
       " 1009,\n",
       " 1010,\n",
       " 1011,\n",
       " 1012,\n",
       " 1013,\n",
       " 1014,\n",
       " 1015,\n",
       " 1016,\n",
       " 1017,\n",
       " 1018,\n",
       " 1019,\n",
       " 1020,\n",
       " 1021,\n",
       " 1022,\n",
       " 1023,\n",
       " 1024,\n",
       " 1025,\n",
       " 1026,\n",
       " 1027,\n",
       " 1028,\n",
       " 1029,\n",
       " 1030,\n",
       " 1031,\n",
       " 1032,\n",
       " 1033,\n",
       " 1034,\n",
       " 1035,\n",
       " 1036,\n",
       " 1037,\n",
       " 1038,\n",
       " 1039,\n",
       " 1040,\n",
       " 1041,\n",
       " 1042,\n",
       " 1043,\n",
       " 1044,\n",
       " 1045,\n",
       " 1046,\n",
       " 1047,\n",
       " 1048,\n",
       " 1049,\n",
       " 1050,\n",
       " 1051,\n",
       " 1052,\n",
       " 1053,\n",
       " 1054,\n",
       " 1055,\n",
       " 1056,\n",
       " 1057,\n",
       " 1058,\n",
       " 1059,\n",
       " 1060,\n",
       " 1061,\n",
       " 1062,\n",
       " 1063,\n",
       " 1064,\n",
       " 1065,\n",
       " 1066,\n",
       " 1067,\n",
       " 1068,\n",
       " 1069,\n",
       " 1070,\n",
       " 1071,\n",
       " 1072,\n",
       " 1073,\n",
       " 1074,\n",
       " 1075,\n",
       " 1076,\n",
       " 1077,\n",
       " 1078,\n",
       " 1079,\n",
       " 1080,\n",
       " 1081,\n",
       " 1082,\n",
       " 1083,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1087,\n",
       " 1088,\n",
       " 1089,\n",
       " 1090,\n",
       " 1091,\n",
       " 1092,\n",
       " 1093,\n",
       " 1094,\n",
       " 1095,\n",
       " 1096,\n",
       " 1097,\n",
       " 1098,\n",
       " 1099,\n",
       " 1100,\n",
       " 1101,\n",
       " 1102,\n",
       " 1103,\n",
       " 1104,\n",
       " 1105,\n",
       " 1106,\n",
       " 1107,\n",
       " 1108,\n",
       " 1109,\n",
       " 1110,\n",
       " 1111,\n",
       " 1112,\n",
       " 1113,\n",
       " 1114,\n",
       " 1115,\n",
       " 1116,\n",
       " 1117,\n",
       " 1118,\n",
       " 1119,\n",
       " 1120,\n",
       " 1121,\n",
       " 1122,\n",
       " 1123,\n",
       " 1124,\n",
       " 1125,\n",
       " 1126,\n",
       " 1127,\n",
       " 1128,\n",
       " 1129,\n",
       " 1130,\n",
       " 1131,\n",
       " 1132,\n",
       " 1133,\n",
       " 1134,\n",
       " 1135,\n",
       " 1136,\n",
       " 1137,\n",
       " 1138,\n",
       " 1139,\n",
       " 1140,\n",
       " 1141,\n",
       " 1142,\n",
       " 1143,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1148,\n",
       " 1149,\n",
       " 1150,\n",
       " 1151,\n",
       " 1152,\n",
       " 1153,\n",
       " 1154,\n",
       " 1155,\n",
       " 1156,\n",
       " 1157,\n",
       " 1158,\n",
       " 1159,\n",
       " 1160,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\"\n",
    "fasta_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\Fasta_files\"\n",
    "\n",
    "def extract_pdb_fasta(pdb_code, pdb_dir, fasta_dir):\n",
    "    \"\"\"Extracts the sequence of a specific chain from a PDB file and writes it as FASTA.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    fasta_filepath = os.path.join(fasta_dir, f\"{pdb_code}.fasta\")\n",
    "\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "\n",
    "    # Only process first model (speed optimization)\n",
    "    model = structure[0]\n",
    "    sequence, observed_residues = [], []\n",
    "\n",
    "    for chain in model:\n",
    "        for residue in chain.get_residues():\n",
    "            if PDB.is_aa(residue):\n",
    "                sequence.append(three_to_one(residue.get_resname()))\n",
    "                observed_residues.append(residue.id[1])\n",
    "\n",
    "    # Write FASTA\n",
    "    fasta_seq = SeqRecord(Seq(\"\".join(sequence)), id=f\"{pdb_code}\", description=\"\")\n",
    "    SeqIO.write(fasta_seq, fasta_filepath, \"fasta\")\n",
    "\n",
    "    return observed_residues\n",
    "    \n",
    "\n",
    "extract_pdb_fasta(pdb_code, pdb_dir, fasta_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae90e3-7c73-4b2b-88ec-1c8306d9b38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
